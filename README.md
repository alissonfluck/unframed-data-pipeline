# üìä Unframed Data Pipeline
Um Projeto Pr√°tico de Engenharia de Dados.

[![Python](https://img.shields.io/badge/Python-3.12.10-blue?logo=python&logoColor=white)](https://www.python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-17.5-blue?logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Version](https://img.shields.io/badge/version-1.0.0-orange)](https://github.com/alissonfluck/SEU_REPOSITORIO/releases)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)


---

## üìë Sum√°rio
- [üìä Unframed Data Pipeline](#-unframed-data-pipeline)
  - [üìë Sum√°rio](#-sum√°rio)
  - [Resumo do Projeto](#resumo-do-projeto)
  - [Stack Tecnol√≥gica](#stack-tecnol√≥gica)
  - [Arquitetura do Pipeline](#arquitetura-do-pipeline)
  - [Objetivos de Neg√≥cio e KPIs](#objetivos-de-neg√≥cio-e-kpis)
    - [üìà KPIs Definidos](#-kpis-definidos)
    - [üìä Exemplos de Sa√≠da](#-exemplos-de-sa√≠da)
      - [Taxa de Ativa√ß√£o Di√°ria (%)](#taxa-de-ativa√ß√£o-di√°ria-)
      - [Comparativo de DAU Leve vs. DAU Core](#comparativo-de-dau-leve-vs-dau-core)
  - [Como Executar o Projeto](#como-executar-o-projeto)
    - [üîß Pr√©-requisitos](#-pr√©-requisitos)
    - [üì• Clonar o reposit√≥rio](#-clonar-o-reposit√≥rio)
    - [üì¶ Instalar depend√™ncias](#-instalar-depend√™ncias)
    - [üîë Configurar vari√°veis de ambiente](#-configurar-vari√°veis-de-ambiente)
    - [‚ñ∂Ô∏è Executar Pipeline](#Ô∏è-executar-pipeline)
  - [Decis√µes T√©cnicas](#decis√µes-t√©cnicas)
  - [Pr√≥ximos Passos](#pr√≥ximos-passos)
    - [Licen√ßa](#licen√ßa)

---

## Resumo do Projeto
Este projeto foi constru√≠do para simular os desafios reais de um **Engenheiro de Dados**, desde a coleta at√© a an√°lise, com foco em demonstrar boas pr√°ticas de **ELT** e gerar **KPIs acion√°veis** para uma plataforma fict√≠cia de streaming, a **Unframed**.

Trata-se de um **pipeline de dados end-to-end** que:
- Extrai eventos de intera√ß√£o do usu√°rio  
- Carrega em um Data Warehouse  
- Transforma para an√°lise e visualiza√ß√£o de m√©tricas de neg√≥cio  

---

## Stack Tecnol√≥gica
- **Linguagem:** Python 3.12.10  
- **Banco de Dados:** PostgreSQL (Data Warehouse)  
- **Bibliotecas Principais:**  
  - `psycopg` ‚Üí conex√£o com o DB  
  - `Faker` ‚Üí gera√ß√£o de dados  
  - `python-dotenv` ‚Üí gerenciamento de segredos  
- **Arquitetura:** Pipeline **ELT** com Data Lake baseado em arquivos (JSON) e pastas de stages:  
  - `landing_zone`  
  - `processed_data`  
  - `archive`  

---

## Arquitetura do Pipeline
O fluxo de dados foi desenhado para ser simples, robusto e observ√°vel, seguindo as etapas cl√°ssicas de um pipeline de dados.  

![Fluxo de Dados](./docs/fluxo_dados_unframed.png "Fluxo de Dados Unframed")

---

## Objetivos de Neg√≥cio e KPIs
Este pipeline transforma dados brutos em insights para responder perguntas cr√≠ticas de neg√≥cio.

### üìà KPIs Definidos
- **DAU (Daily Active Users)**  
  Contagem de usu√°rios √∫nicos que realizaram eventos no dia.  
  *Mede engajamento e base ativa da plataforma.*  

- **Taxa de Convers√£o**  
  `(Usu√°rios criaram conta / Visitantes √∫nicos) * 100`  
  *Avalia efici√™ncia do funil de aquisi√ß√£o.*  

- **Taxa de Ativa√ß√£o**  
  `(Novos usu√°rios que deram play / Total de novos usu√°rios) * 100`  
  *Verifica se usu√°rios est√£o extraindo valor logo ap√≥s cadastro.*  

- **Funil de Login**  
  Eventos `login_succeeded` vs `login_failed`  
  *Identifica atritos ou falhas na autentica√ß√£o.*  

---

### üìä Exemplos de Sa√≠da

#### Taxa de Ativa√ß√£o Di√°ria (%)
![Daily Active Users](./docs/taxa_de_ativa√ß√£o.png)

<details>
<summary>üìú Query SQL da Taxa de Ativa√ß√£o</summary>

```sql
WITH signups_by_day AS (
    SELECT DISTINCT
        CAST(envelope_eventtimestamp AS DATE) AS signup_date,
        payload_userid
    FROM user_created
),
playbacks_by_day AS (
    SELECT DISTINCT
        CAST(envelope_eventtimestamp AS DATE) AS playback_date,
        payload_userid
    FROM playback_started
)
SELECT
    s.signup_date AS dia,
    ROUND(
        (CAST(COUNT(p.payload_userid) AS NUMERIC) / NULLIF(CAST(COUNT(s.payload_userid) AS NUMERIC), 0)) * 100.0,
        2
    ) AS taxa_de_ativacao_percent
FROM
    signups_by_day s
LEFT JOIN
    playbacks_by_day p ON s.payload_userid = p.payload_userid AND s.signup_date = p.playback_date
GROUP BY
    s.signup_date
ORDER BY
    s.signup_date ASC;
```
</details>

---

#### Comparativo de DAU Leve vs. DAU Core
![Daily Active Users](./docs/daily_active_users.png)

<details>
<summary>üìú Query SQL DAU Leve vs. Core</summary>

```sql
WITH dau_leve AS (
    SELECT
        CAST(envelope_eventtimestamp AS DATE) AS dia,
        COUNT(DISTINCT payload_userid) AS total_users
    FROM (
        SELECT envelope_eventtimestamp, payload_userid FROM user_created
        UNION ALL
        SELECT envelope_eventtimestamp, payload_userid FROM login_succeeded
        UNION ALL
        SELECT envelope_eventtimestamp, payload_userid FROM playback_started
    ) AS all_events
    GROUP BY dia
),
dau_core AS (
    SELECT
        CAST(envelope_eventtimestamp AS DATE) AS dia,
        COUNT(DISTINCT payload_userid) AS total_users
    FROM playback_started
    GROUP BY dia
)
SELECT
    dl.dia,
    dl.total_users AS dau_leve,
    COALESCE(dc.total_users, 0) AS dau_core
FROM
    dau_leve dl
LEFT JOIN
    dau_core dc ON dl.dia = dc.dia
ORDER BY
    dl.dia ASC;
```
</details>

---

## Como Executar o Projeto

### üîß Pr√©-requisitos
- Python 3.8+  
- PostgreSQL instalado e rodando  
- Banco de dados criado (`unframed_dw`)  

---

### üì• Clonar o reposit√≥rio
```bash
git clone [URL_DO_SEU_REPOSITORIO]
cd [NOME_DA_PASTA_DO_PROJETO]
```

### üì¶ Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### üîë Configurar vari√°veis de ambiente
Crie um arquivo chamado `.env` na raiz do projeto e adicione suas credenciais:

```ini
# Arquivo: .env
DB_USER="seu_usuario"
DB_PASSWORD="sua_senha"
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="unframed_dw"
```

‚ö†Ô∏è **Importante:** adicione o `.env` ao `.gitignore` para n√£o expor suas credenciais.

---

### ‚ñ∂Ô∏è Executar Pipeline
1. **Gerar dados brutos na `landing_zone`**
   ```bash
   python event_generatorv3.py
   ```

2. **Transformar dados para `processed_data`**
   ```bash
   python ingest_and_process.py
   ```

3. **Carregar dados no PostgreSQL**
   ```bash
   python load_to_dw.py
   ```

---

## Decis√µes T√©cnicas
- **Gera√ß√£o de Dados** (`event_generatorv3.py`)  
  Simula√ß√£o realista de personas (para garantir que a l√≥gica de "costura" de IDs fosse testada com dados coerentes, simulando jornadas reais de convers√£o), enriquecimento de URLs, controle temporal.

- **Transforma√ß√£o** (`ingest_and_process.py`)  
  Gerenciamento de estado, para garantir que o pipeline pudesse, no futuro, lidar com m√∫ltiplos arquivos de forma idempotente, sabendo o que j√° foi processado, al√©m disso, flattening de JSON para maior clareza.

- **Carga** (`load_to_dw.py`)  
  Gerenciamento seguro de segredos via `.env`, carga com estrat√©gia de *full refresh* para a simplicidade do MVP.

---

## Pr√≥ximos Passos
- Orquestra√ß√£o com **Apache Airflow**  
- Migrar para **Data Warehouse em nuvem** (BigQuery, Snowflake)  
- Adicionar **testes de qualidade de dados** (dbt, Great Expectations)  

---

### Licen√ßa

MIT License ¬© 2025 Alisson Fluck